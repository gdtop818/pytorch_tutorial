{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
    "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
    "holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28381492.0\n",
      "1 22915732.0\n",
      "2 22275188.0\n",
      "3 22675172.0\n",
      "4 21835244.0\n",
      "5 18752290.0\n",
      "6 13996350.0\n",
      "7 9264500.0\n",
      "8 5662078.0\n",
      "9 3391066.0\n",
      "10 2083559.75\n",
      "11 1360151.875\n",
      "12 953874.0\n",
      "13 715977.625\n",
      "14 567213.8125\n",
      "15 467190.3125\n",
      "16 395062.4375\n",
      "17 339958.4375\n",
      "18 296030.03125\n",
      "19 259944.5\n",
      "20 229666.359375\n",
      "21 203892.265625\n",
      "22 181777.171875\n",
      "23 162631.625\n",
      "24 145928.421875\n",
      "25 131300.34375\n",
      "26 118417.8046875\n",
      "27 107039.4921875\n",
      "28 96969.6328125\n",
      "29 88039.8515625\n",
      "30 80113.4609375\n",
      "31 73027.0234375\n",
      "32 66678.1796875\n",
      "33 60973.5\n",
      "34 55835.40625\n",
      "35 51200.37890625\n",
      "36 47008.58203125\n",
      "37 43215.48828125\n",
      "38 39774.12109375\n",
      "39 36645.6796875\n",
      "40 33802.0546875\n",
      "41 31211.03125\n",
      "42 28846.119140625\n",
      "43 26682.685546875\n",
      "44 24702.517578125\n",
      "45 22887.583984375\n",
      "46 21222.892578125\n",
      "47 19693.5546875\n",
      "48 18288.181640625\n",
      "49 16996.474609375\n",
      "50 15806.67578125\n",
      "51 14711.568359375\n",
      "52 13700.8515625\n",
      "53 12766.8701171875\n",
      "54 11903.2705078125\n",
      "55 11103.830078125\n",
      "56 10363.6904296875\n",
      "57 9678.041015625\n",
      "58 9042.0751953125\n",
      "59 8451.630859375\n",
      "60 7903.74560546875\n",
      "61 7394.333984375\n",
      "62 6920.91845703125\n",
      "63 6480.8115234375\n",
      "64 6070.8076171875\n",
      "65 5689.12255859375\n",
      "66 5333.87548828125\n",
      "67 5002.6953125\n",
      "68 4693.74609375\n",
      "69 4405.4130859375\n",
      "70 4136.1904296875\n",
      "71 3884.99267578125\n",
      "72 3650.324462890625\n",
      "73 3430.941162109375\n",
      "74 3225.716064453125\n",
      "75 3033.7451171875\n",
      "76 2854.042236328125\n",
      "77 2685.828369140625\n",
      "78 2528.268798828125\n",
      "79 2380.60400390625\n",
      "80 2242.123046875\n",
      "81 2112.35009765625\n",
      "82 1990.5753173828125\n",
      "83 1876.29296875\n",
      "84 1769.149169921875\n",
      "85 1668.419677734375\n",
      "86 1573.8387451171875\n",
      "87 1484.993408203125\n",
      "88 1401.46435546875\n",
      "89 1322.978271484375\n",
      "90 1249.2681884765625\n",
      "91 1179.8607177734375\n",
      "92 1114.57373046875\n",
      "93 1053.158203125\n",
      "94 995.3382568359375\n",
      "95 940.8904418945312\n",
      "96 889.6611328125\n",
      "97 841.3536376953125\n",
      "98 795.8282470703125\n",
      "99 752.9290771484375\n",
      "100 712.49755859375\n",
      "101 674.369873046875\n",
      "102 638.44580078125\n",
      "103 604.5172729492188\n",
      "104 572.4974975585938\n",
      "105 542.28466796875\n",
      "106 513.7514038085938\n",
      "107 486.8041076660156\n",
      "108 461.3788757324219\n",
      "109 437.36517333984375\n",
      "110 414.6653137207031\n",
      "111 393.205078125\n",
      "112 372.92486572265625\n",
      "113 353.75347900390625\n",
      "114 335.6258544921875\n",
      "115 318.4943542480469\n",
      "116 302.27703857421875\n",
      "117 286.9253845214844\n",
      "118 272.3949279785156\n",
      "119 258.64898681640625\n",
      "120 245.643798828125\n",
      "121 233.33306884765625\n",
      "122 221.67649841308594\n",
      "123 210.62286376953125\n",
      "124 200.1467742919922\n",
      "125 190.22280883789062\n",
      "126 180.81871032714844\n",
      "127 171.90660095214844\n",
      "128 163.48080444335938\n",
      "129 155.47361755371094\n",
      "130 147.87217712402344\n",
      "131 140.663818359375\n",
      "132 133.8258056640625\n",
      "133 127.33905029296875\n",
      "134 121.18673706054688\n",
      "135 115.35401916503906\n",
      "136 109.8104248046875\n",
      "137 104.54525756835938\n",
      "138 99.54460906982422\n",
      "139 94.79864501953125\n",
      "140 90.2899398803711\n",
      "141 86.00724792480469\n",
      "142 81.94416046142578\n",
      "143 78.07952880859375\n",
      "144 74.40304565429688\n",
      "145 70.90862274169922\n",
      "146 67.58494567871094\n",
      "147 64.4262924194336\n",
      "148 61.42304992675781\n",
      "149 58.57001876831055\n",
      "150 55.85713577270508\n",
      "151 53.2718505859375\n",
      "152 50.8109016418457\n",
      "153 48.46955490112305\n",
      "154 46.24294662475586\n",
      "155 44.121482849121094\n",
      "156 42.10429000854492\n",
      "157 40.18768310546875\n",
      "158 38.357940673828125\n",
      "159 36.6142692565918\n",
      "160 34.95450973510742\n",
      "161 33.37392807006836\n",
      "162 31.869892120361328\n",
      "163 30.4360408782959\n",
      "164 29.06979751586914\n",
      "165 27.76983642578125\n",
      "166 26.527481079101562\n",
      "167 25.343393325805664\n",
      "168 24.215412139892578\n",
      "169 23.139585494995117\n",
      "170 22.113786697387695\n",
      "171 21.13482666015625\n",
      "172 20.202056884765625\n",
      "173 19.313106536865234\n",
      "174 18.46407699584961\n",
      "175 17.65300941467285\n",
      "176 16.87971305847168\n",
      "177 16.14095687866211\n",
      "178 15.436295509338379\n",
      "179 14.763497352600098\n",
      "180 14.122227668762207\n",
      "181 13.509645462036133\n",
      "182 12.924723625183105\n",
      "183 12.3656644821167\n",
      "184 11.831863403320312\n",
      "185 11.321435928344727\n",
      "186 10.834342956542969\n",
      "187 10.368849754333496\n",
      "188 9.924469947814941\n",
      "189 9.499777793884277\n",
      "190 9.094539642333984\n",
      "191 8.706854820251465\n",
      "192 8.336368560791016\n",
      "193 7.9816670417785645\n",
      "194 7.643243789672852\n",
      "195 7.3194499015808105\n",
      "196 7.009687900543213\n",
      "197 6.713377952575684\n",
      "198 6.430575370788574\n",
      "199 6.159946441650391\n",
      "200 5.901224136352539\n",
      "201 5.653564929962158\n",
      "202 5.416520118713379\n",
      "203 5.189719200134277\n",
      "204 4.972939968109131\n",
      "205 4.765582084655762\n",
      "206 4.566841125488281\n",
      "207 4.376670837402344\n",
      "208 4.194826602935791\n",
      "209 4.0209550857543945\n",
      "210 3.8544602394104004\n",
      "211 3.6947438716888428\n",
      "212 3.5421714782714844\n",
      "213 3.395972490310669\n",
      "214 3.256115198135376\n",
      "215 3.1221935749053955\n",
      "216 2.9935834407806396\n",
      "217 2.870666742324829\n",
      "218 2.7529921531677246\n",
      "219 2.6402406692504883\n",
      "220 2.5323638916015625\n",
      "221 2.4289331436157227\n",
      "222 2.3297107219696045\n",
      "223 2.234966516494751\n",
      "224 2.143908977508545\n",
      "225 2.0568578243255615\n",
      "226 1.97323477268219\n",
      "227 1.8932220935821533\n",
      "228 1.816420316696167\n",
      "229 1.7429254055023193\n",
      "230 1.6725540161132812\n",
      "231 1.6050363779067993\n",
      "232 1.5403341054916382\n",
      "233 1.4783425331115723\n",
      "234 1.4188148975372314\n",
      "235 1.361833930015564\n",
      "236 1.3070658445358276\n",
      "237 1.2546809911727905\n",
      "238 1.204376220703125\n",
      "239 1.1562235355377197\n",
      "240 1.109877109527588\n",
      "241 1.0657727718353271\n",
      "242 1.0232089757919312\n",
      "243 0.9823518395423889\n",
      "244 0.94334477186203\n",
      "245 0.9056828618049622\n",
      "246 0.86971116065979\n",
      "247 0.8351601362228394\n",
      "248 0.802087664604187\n",
      "249 0.7703071236610413\n",
      "250 0.7398387789726257\n",
      "251 0.7105053067207336\n",
      "252 0.6824800968170166\n",
      "253 0.6555382609367371\n",
      "254 0.6297053694725037\n",
      "255 0.6048945188522339\n",
      "256 0.5810689926147461\n",
      "257 0.5582179427146912\n",
      "258 0.5363524556159973\n",
      "259 0.5152369141578674\n",
      "260 0.49503588676452637\n",
      "261 0.4755750298500061\n",
      "262 0.4569464325904846\n",
      "263 0.43909215927124023\n",
      "264 0.4219028949737549\n",
      "265 0.40545210242271423\n",
      "266 0.38961419463157654\n",
      "267 0.3744337856769562\n",
      "268 0.3598012626171112\n",
      "269 0.3458222448825836\n",
      "270 0.33235523104667664\n",
      "271 0.319443941116333\n",
      "272 0.3070591986179352\n",
      "273 0.2951362729072571\n",
      "274 0.2836652398109436\n",
      "275 0.272723913192749\n",
      "276 0.2621491253376007\n",
      "277 0.25198492407798767\n",
      "278 0.24227218329906464\n",
      "279 0.2329643815755844\n",
      "280 0.22396844625473022\n",
      "281 0.21530380845069885\n",
      "282 0.20705047249794006\n",
      "283 0.1990956962108612\n",
      "284 0.1913987398147583\n",
      "285 0.1839989274740219\n",
      "286 0.17697183787822723\n",
      "287 0.1701878160238266\n",
      "288 0.16365471482276917\n",
      "289 0.15739399194717407\n",
      "290 0.15138106048107147\n",
      "291 0.14556491374969482\n",
      "292 0.14000719785690308\n",
      "293 0.13465377688407898\n",
      "294 0.12948691844940186\n",
      "295 0.12455473095178604\n",
      "296 0.11980939656496048\n",
      "297 0.11523754149675369\n",
      "298 0.11084970086812973\n",
      "299 0.10661584883928299\n",
      "300 0.10255853831768036\n",
      "301 0.09868083894252777\n",
      "302 0.09494601935148239\n",
      "303 0.09132811427116394\n",
      "304 0.08789902925491333\n",
      "305 0.08455269038677216\n",
      "306 0.08134390413761139\n",
      "307 0.07828717678785324\n",
      "308 0.07531080394983292\n",
      "309 0.07247070968151093\n",
      "310 0.06974273920059204\n",
      "311 0.06711040437221527\n",
      "312 0.06456700712442398\n",
      "313 0.06213882565498352\n",
      "314 0.05978541821241379\n",
      "315 0.057557106018066406\n",
      "316 0.05541228502988815\n",
      "317 0.053305089473724365\n",
      "318 0.05130593478679657\n",
      "319 0.049400437623262405\n",
      "320 0.04754475876688957\n",
      "321 0.045774366706609726\n",
      "322 0.0440724641084671\n",
      "323 0.04241230711340904\n",
      "324 0.04083605855703354\n",
      "325 0.03930798918008804\n",
      "326 0.03785458207130432\n",
      "327 0.036426544189453125\n",
      "328 0.0350743904709816\n",
      "329 0.03376278653740883\n",
      "330 0.03249382972717285\n",
      "331 0.03130049258470535\n",
      "332 0.03013507090508938\n",
      "333 0.029039308428764343\n",
      "334 0.027964048087596893\n",
      "335 0.026923250406980515\n",
      "336 0.025934098288416862\n",
      "337 0.02497657760977745\n",
      "338 0.024054937064647675\n",
      "339 0.023168344050645828\n",
      "340 0.02231854572892189\n",
      "341 0.021502571180462837\n",
      "342 0.020725512877106667\n",
      "343 0.019967008382081985\n",
      "344 0.019231406971812248\n",
      "345 0.018529383465647697\n",
      "346 0.0178493931889534\n",
      "347 0.017205148935317993\n",
      "348 0.016575226560235023\n",
      "349 0.015977801755070686\n",
      "350 0.015390856191515923\n",
      "351 0.014834709465503693\n",
      "352 0.014299016445875168\n",
      "353 0.013780727982521057\n",
      "354 0.013273835182189941\n",
      "355 0.012797022238373756\n",
      "356 0.01233366597443819\n",
      "357 0.011897338554263115\n",
      "358 0.011473232880234718\n",
      "359 0.011067214421927929\n",
      "360 0.010671175085008144\n",
      "361 0.010289461351931095\n",
      "362 0.009923174977302551\n",
      "363 0.009573482908308506\n",
      "364 0.009230577386915684\n",
      "365 0.008906257338821888\n",
      "366 0.008582874201238155\n",
      "367 0.008286085911095142\n",
      "368 0.007994590327143669\n",
      "369 0.007713918574154377\n",
      "370 0.007449498400092125\n",
      "371 0.007190377451479435\n",
      "372 0.006942400708794594\n",
      "373 0.006700717844069004\n",
      "374 0.006469994783401489\n",
      "375 0.006250156555324793\n",
      "376 0.006033633835613728\n",
      "377 0.0058265505358576775\n",
      "378 0.005624310113489628\n",
      "379 0.005435069091618061\n",
      "380 0.0052476916462183\n",
      "381 0.005064785946160555\n",
      "382 0.004895098973065615\n",
      "383 0.004729608539491892\n",
      "384 0.004571494646370411\n",
      "385 0.004416031762957573\n",
      "386 0.004265241790562868\n",
      "387 0.004127185791730881\n",
      "388 0.003989985212683678\n",
      "389 0.0038559320382773876\n",
      "390 0.0037252800539135933\n",
      "391 0.0036056479439139366\n",
      "392 0.003485427238047123\n",
      "393 0.003373361425474286\n",
      "394 0.0032628681510686874\n",
      "395 0.003159724408760667\n",
      "396 0.0030551510863006115\n",
      "397 0.0029561505652964115\n",
      "398 0.002861646469682455\n",
      "399 0.0027682678773999214\n",
      "400 0.0026810590643435717\n",
      "401 0.0025964579544961452\n",
      "402 0.0025146196130663157\n",
      "403 0.002436375943943858\n",
      "404 0.002357199089601636\n",
      "405 0.002284102840349078\n",
      "406 0.0022122629452496767\n",
      "407 0.002143598860129714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 0.002075634431093931\n",
      "409 0.0020120148546993732\n",
      "410 0.0019498651381582022\n",
      "411 0.0018913953099399805\n",
      "412 0.0018332985928282142\n",
      "413 0.001777996658347547\n",
      "414 0.001725560869090259\n",
      "415 0.0016751803923398256\n",
      "416 0.0016239627730101347\n",
      "417 0.001578743802383542\n",
      "418 0.0015317507786676288\n",
      "419 0.0014877215726301074\n",
      "420 0.0014441825915127993\n",
      "421 0.0014024110278114676\n",
      "422 0.0013616678770631552\n",
      "423 0.0013251120690256357\n",
      "424 0.0012850005878135562\n",
      "425 0.0012479388387873769\n",
      "426 0.001212829607538879\n",
      "427 0.0011789383133873343\n",
      "428 0.0011464550625532866\n",
      "429 0.0011148572666570544\n",
      "430 0.0010833883425220847\n",
      "431 0.0010518708731979132\n",
      "432 0.00102336669806391\n",
      "433 0.0009952818509191275\n",
      "434 0.0009669457795098424\n",
      "435 0.0009415525128133595\n",
      "436 0.0009173309081234038\n",
      "437 0.0008919470710679889\n",
      "438 0.00086927943630144\n",
      "439 0.0008469211752526462\n",
      "440 0.0008246144861914217\n",
      "441 0.0008029320160858333\n",
      "442 0.0007817996665835381\n",
      "443 0.0007623230339959264\n",
      "444 0.0007423057686537504\n",
      "445 0.0007225826848298311\n",
      "446 0.0007044053636491299\n",
      "447 0.0006873742677271366\n",
      "448 0.0006703204708173871\n",
      "449 0.0006535904831252992\n",
      "450 0.0006388495676219463\n",
      "451 0.0006229067221283913\n",
      "452 0.0006075077108107507\n",
      "453 0.0005920847179368138\n",
      "454 0.0005782609805464745\n",
      "455 0.0005638606380671263\n",
      "456 0.0005505774170160294\n",
      "457 0.0005377259803935885\n",
      "458 0.0005251720431260765\n",
      "459 0.0005128745106048882\n",
      "460 0.0005007673753425479\n",
      "461 0.0004901037318632007\n",
      "462 0.0004787894431501627\n",
      "463 0.00046780321281403303\n",
      "464 0.00045682128984481096\n",
      "465 0.0004472292785067111\n",
      "466 0.0004366055072750896\n",
      "467 0.0004273479280527681\n",
      "468 0.0004178747476544231\n",
      "469 0.0004085949040018022\n",
      "470 0.0003990576951764524\n",
      "471 0.00039076892426237464\n",
      "472 0.0003816754906438291\n",
      "473 0.00037334844819270074\n",
      "474 0.0003658882051240653\n",
      "475 0.00035836914321407676\n",
      "476 0.00035070188459940255\n",
      "477 0.00034339496050961316\n",
      "478 0.0003361748531460762\n",
      "479 0.0003293342306278646\n",
      "480 0.0003219944192096591\n",
      "481 0.0003151519631501287\n",
      "482 0.0003087440854869783\n",
      "483 0.00030283030355349183\n",
      "484 0.00029651346267201006\n",
      "485 0.0002903647837229073\n",
      "486 0.00028453097911551595\n",
      "487 0.0002790573635138571\n",
      "488 0.00027381267864257097\n",
      "489 0.0002683557104319334\n",
      "490 0.0002626698696985841\n",
      "491 0.000256922998232767\n",
      "492 0.0002523849543649703\n",
      "493 0.00024756748462095857\n",
      "494 0.00024331647728104144\n",
      "495 0.0002379209327045828\n",
      "496 0.00023323408095166087\n",
      "497 0.00022895782603882253\n",
      "498 0.00022462446941062808\n",
      "499 0.00022064236691221595\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
